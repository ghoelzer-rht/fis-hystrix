:noaudio:
:scrollbar:
:data-uri:
:toc2:

== Lab Guide - Deploying a Microservice Gateway using Kubeflix & Hysterix

In this lab, we will create a Gateway Microservice Pattern implementation of a simple Dictionary Definition service using the Kubeflix API Implementation for Service Discovery and Routing.  Additionally, we will use Hystrix for Microservice monitoring and implementation of a Circuit Breaker pattern.  The Microservice implementations are using Red Hat Fuse Integration Services (FIS) 2.0, and use relatively simple REST and Java DSL, along with Java POJO and Service implementations.  FIS 2.0 is an integrated Java Framework that's highly suitable for Microservice implementations, and focused on real-time/event-driven integration patterns.  For more background on all the FIS 2.0 capabilities see - https://developers.redhat.com/blog/2017/02/21/announcing-fuse-for-agile-integration-on-the-cloud-fis-2-0-release/

*Fork/Clone this Repo into a local working directory to complete the Lab Exercises*

:numbered:

== Deploy Base Microservices to OpenShift

1. Login to the remote OpenShift environment (Get instructions from your OpenShift admin)

1. Create a new project with a unique name or ensure an existing OpenShift project is selected 
+
    oc new-project fis-hysterix-<unique number>
    or
    oc project <your-project-name>

1. From the base of where you cloned your git repo, execute the following commands:
+
[source,bash]
----
oc process -f support/templates/gateway.json | oc create -f-

oc process -f support/templates/definition.json | oc create -f-
----
+
From the OpenShift Web Console, select the project you have created or are using.  You should see that two s2i builds have kicked-off for the *gateway* and *definition components*, which will deploy when complete as shown below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-deployment1.png[]
+

Expand the *gateway* microservice deployment, and click/select the created Route.  This should open a Swagger UI/API Explorer for the deployed *definition* microservice, verify the operation of the definition API as shown below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-deployment2.png[]

== Implement Hysterix Dashboard and Turbine Server
Now let’s add the Hysterix Dashboard and Turbine Server components to monitor the gateway and definition microservices within our project.

1. Back at the command line, navigate to the root of the git project and execute the following *oc* command:
+
[source,bash]
----
oc process -f support/templates/kubeflix.json | oc create -f-
----
+
You should now see these components deployed and running in your project after the supporting Container Images were successfully pulled as shown below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-hystrix1.png[]

1. Open the Hysterix Dashboard and generate some API activity using the Swagger API Console.  Start by opening browser tab at the hystrix-dashboard Route Http Endpoint as show below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-turbine1.png[]
+
Click the Monitor Stream button to display the Dashboard and hit the definition API a bunch of times from the Swagger API Console and you should see something like what is displayed below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-dashboard1.png[]
+
Now let's try out the *circuit breaker* for the *definition* API.  Go back to the definition deployment within the Web Console, expand the view, and on the diplayed pod, click the down arrow scaling it to *Zero* pods as shown below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-hystrix2.png[]
+
Now hit the definition API a bunch of times from the Swagger API Console and you should now see that the circuit breaker has *opened* due to the number of failures of the definition API as *displayed below*.  When the virtual "circuit" between the gateway/service router and the target microservice has been broken (e.g. open), the gateway immediately responds with a failure.  However, the gateway will route a request every so often at a set interval, testing the microservice and will close the circuit after a number of consecutive successful responses.
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-dashboard2.png[]
+
Go back to the definition deployment within the Web Console and on the diplayed pod, click the up arrow scaling it to *One*.  After the definition Pod Health Check succeeds, hit the definition API a bunch of times from the Swagger API Console.  You should see that the circuit breaker is now *open* again and successful responses are returned.

== *** Break:  Review JBoss Developer Studio and Project Code ***
Now let's take a short break from Lab Exercises so we can look at the Developer IDE provided by JBoss Developer Studio.  *Make sure to have installed JBDS*. The package can be found https://access.redhat.com/jbossnetwork/restricted/softwareDownload.html?softwareId=51401[here].  If you don't have direct access, work with your OpenShift/Red Hat Sysadmin.  Download the settings.xml file from https://view.highspot.com/viewer/58e3afde3e255f7a216dc4ce[here] and copy it to your .m2 directory. Make sure to
replace <localRepository>….</localRepository> with the path your .m2 directory. Additionally, ensure the filename
is all lowercase (e.g. not Settings.xml).

Items to go over include:

1. Installation of the Fuse/Integration pack from the additional Red Hat Plugins provided
2. Import the existing Maven Lab Project into the IDE
3. Review the Fuse Spring and Camel Java Route and Service Implementations
4. Use the Openshift Explorer to connect to your OpenShift Lab Environment

== HA, Auto-Scaling, and Prod Deployment Considerations
This exercise will be a combination of a discussion around best practices, simple demonstrations using our Gateway Microservice example, and use of simple utilities to simulate load to validate configurations.  For the purposes of this lab, we will use Apache Benchmark to simulate load, if not already installed it can be found https://www.apachelounge.com/download/[here] (yes, you have to install the apache httpd binaries).

1. By default, Kubernetes Replication Controllers will ensure the requested number of Pods are running in the Cluster, and as part of our previous labs we saw that Rolling/No Downtime Deployments are supported.  Some additional considerations would be to ensure *two or more* Pods for components that require HA are specified.  Let's do this right now, and scale our *gateway* deployment:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-ha1.png[]
+
Now let's simulate some load against the gateway and definition microservice using the following *ab* command below, get the URL from the Swagger UI used in the previous Lab Exercise:
+
[source,bash]
----
ab -n 100000 -c 4 -s 5 -k -r http://gateway-fis-hysterix-lab01.cloudapps.keyvan.com/api/definition/camel
----
+
This will generate a 100K requests using 4 sessions against our API.  Open the Hystrix Dashboard, and you can watch/monitor the activity.  Let's now "simulate" an outage by killing one of the Pods.  In the Web Console, click in the middle of the Pods graphic to display the running Pods.  While ensuring your *ab* command is still running, select one of the running Pods and then from the *Actions* Drop Down, select *Delete* to terminate it.  Go back to the Project Overview, and you should see a new Pod starting, and your *ab* command still running.  Watch the Hystrix Dashboard to see the stats/graphic change while the new Gateway Pod starts up.

1. Now lets review some of the key configurations we already have in place necssary for an HA Deployment such as *Readiness and Liveness Probes*.
+
Open the *definition deployment configuration* from the Web Console, and then select *Edit Health Checks* from the *Actions* Drop Down.  You will see the following displayed:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-ha2.png[]
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-ha3.png[]
+
The *Readiness Probe* ensures that no traffic will be routed to the Pod until the Container/Application is ready to service traffic.  This is an important consideration given that moment a Pod/Container is in a *running* state, the K8S Service will attempt to send any requests to it.  The *Readiness Probe* ensures the Application within the Container is also in a running state and is ready to service requests.
+
The *Liveness Probe* has a slightly different function.  Should the Application within the Container become hung or unresponsive (default is 3 failures), the Pod/Container will be killed allowing the K8S Replication Controller to spin up a new Pod/Container. Again, having *two or more* Pods is necessary for a true HA deployment, in case of a Liveness Probe failure.

1. Now lets define some Resource Requests/Limits for the *definition* microservice.  These will be needed/used to enable *Auto-Scaling* in the next step.  We will also have a brief discussion on the K8S *Quality of Service* assigned to Pods.
+
Open the *definition deployment configuration* from the Web Console, and then select *Edit Resource Limits* from the *Actions* Drop Down.  You will see the following displayed:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-ha4.png[]
+
Update the CPU and Memory Requests and Limits as shown below.  This will instruct the K8S Scheduler to find a Node Host with the Resources specified, and reserve the requested resources for Pods that are associated with this Deployment Configuration on the Node Hosts where they are running.  In addition, Auto-Scaling requires that a *CPU Request is specified*:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-ha5.png[]
+
Save your changes, you will then see that a Rolling Deployment of the updated configuration is triggered, as the K8S Scheduler creates a new Pod with the desired Resource Requests/Limits.

1. Now let's setup *Auto-Scaling* for our *definition* Deployment Configuration.  Currently, the only metric "out of the box" for Auto-Scaling within K8S is *CPU*.  
+
Make sure the *definition deployment configuration* is selected in the Web Console, then select *Add Autoscaler* from the *Actions Drop Down*.  Create/Save the Autoscaler definition as shown below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-ha6.png[]
+
Now let's create some traffic and trigger a Scaling event.  After an initial Scaling Event is triggered, Scaling events ocurr only on predefined intervals (default 5 min) to prevent "flapping" or erratic scale up/down of resources.  Execute the *ab* command again, this time doubling the number of total requests:
+
[source,bash]
----
ab -n 200000 -c 4 -s 5 -k -r http://gateway-fis-hysterix-lab01.cloudapps.keyvan.com/api/definition/camel
----
+
After 30-60 seconds, you should see that a Scaling event was triggered in the Web Console as shown below:
+
image::https://github.com/ghoelzer-rht/fis-hystrix/blob/master/images/fis-lab-ha7.png[]





